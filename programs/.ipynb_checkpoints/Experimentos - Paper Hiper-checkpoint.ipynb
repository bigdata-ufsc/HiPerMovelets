{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# d = '/Users/tarlis/git/HIPERMovelets'\n",
    "d = '/home/tarlis/Research/tarlis'\n",
    "# d = 'C:\\\\Users\\\\05652761741\\\\Documents\\\\tarlis'\n",
    "data_folder = os.path.join(d, 'data')\n",
    "\n",
    "res_path    = os.path.join(d, 'results')\n",
    "prog_path   = os.path.join(d, 'programs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testesMARC(data, res_path, prefix, dataset, toprint=False):\n",
    "#     global prog_path #THREADS, GIG\n",
    "#     prefix = prefix + '_T' + THREADS + '_' + GIG\n",
    "    marcpath = os.path.join(prog_path, 'marc')\n",
    "    k = 5\n",
    "    \n",
    "    if dataset is not '':\n",
    "        ds = '-' + dataset\n",
    "        dstrain=dataset+\"_train.csv\"\n",
    "        dstest=dataset+\"_test.csv\"\n",
    "    else:\n",
    "        ds = ''\n",
    "        dstrain=\"train.csv\"\n",
    "        dstest=\"test.csv\"\n",
    "    \n",
    "    k_MARC(k, data, res_path, prefix, 'MARC'+ds, train=dstrain, test=dstest, print_only=toprint, prg_path=marcpath)\n",
    "    \n",
    "def testesPOIFREQ(k, data_folder, res_path, prefix, dataset):\n",
    "    for i in range(1,k+1):\n",
    "        CMD = \"python3 automatize/poifreq/poifreq.py \\\"npoi\\\" \\\"1,2,3\\\" \\\"\"+dataset+\"\\\" \\\"\\\" \"\n",
    "        CMD = CMD + \"\\\"\"+os.path.join(data_folder, 'run'+str(i))+\"\\\" \"\n",
    "        CMD = CMD + \"\\\"\"+os.path.join(res_path, prefix, 'run'+str(i), 'POIFREQ-'+dataset)+\"\\\"\"\n",
    "        print(CMD)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Run Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatize.run import run, MARC, k_MARC\n",
    "\n",
    "def k_run(k, data_folder, res_path, prefix, folder, descriptor, version = 'hiper', ms = False, Ms = False, extra=False, \n",
    "        java_opts='', jar_name='HIPERMovelets', n_threads=1, prg_path='./', print_only=False, keep_folder=2):\n",
    "    \n",
    "    j = 0\n",
    "    k = 5\n",
    "    for x in range(j,k):\n",
    "        subpath_data = os.path.join(data_folder, 'run'+str(x+1))\n",
    "        subpath_rslt = os.path.join(res_path,    'run'+str(x+1))\n",
    "        run(subpath_data, subpath_rslt, prefix, folder, descriptor, version, ms, Ms, extra, \n",
    "        java_opts, jar_name, n_threads, prg_path, print_only, keep_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "import glob\n",
    "cbk = '▢'\n",
    "s = ('<ul>')\n",
    "for root, dirs, files in os.walk(\"./scripts\"):\n",
    "    path = root.split(os.sep)\n",
    "    for d in dirs[1:]:\n",
    "        s += ('<li>'+cbk+' ⇨ '+ os.path.basename(d).split('-')[0] + ':')\n",
    "        s += ('<ul>')\n",
    "        for file in glob.glob(\"./scripts/\"+d+'/*.sh'):\n",
    "            name = os.path.basename(file)[5:-3]\n",
    "            name = name.replace('splice-junction-gene-sequences','splice_junction_gene_sequences')\n",
    "            name = name.split('-')\n",
    "#             print(name)\n",
    "            if 'bility' in name:\n",
    "                s += ('<li>'+cbk+' ⇨ Scalability - '+ name[1] +'</li>')\n",
    "            else:\n",
    "                s += ('<li>1'+cbk+' - 2'+cbk+' - 3'+cbk+' - 4'+cbk+' - 5'+cbk+' ⇨ '+ name[1] +' - '+ name[2] +'</li>')\n",
    "        s += ('</ul></li>')\n",
    "s += ('</ul>')\n",
    "HTML(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scripts(method, datasets):\n",
    "    \n",
    "    THREADS = '4'#'3'\n",
    "    GIG = '60'\n",
    "    TEST_PATH   = method+'-5fold'+ '_' + THREADS + 'T_' + GIG+'G'\n",
    "    DESC_PATH   = os.path.join(data_folder, '5fold', 'descriptors')\n",
    "    results     = os.path.join(res_path, TEST_PATH)\n",
    "    # -----------------------------\n",
    "    \n",
    "    print_only = True\n",
    "    k = 5\n",
    "\n",
    "#     print('#!/bin/bash')\n",
    "    print('# To run all: ' + method)\n",
    "    for key in datasets:\n",
    "        ds = key.split(sep='.')[0]\n",
    "        desc = key.split(sep='.')[1]\n",
    "        for var in datasets[key]:\n",
    "                        \n",
    "            prefix      = ds.capitalize()\n",
    "            data        = os.path.join(data_folder, '5fold', ds)\n",
    "            json        = os.path.join(DESC_PATH, desc)\n",
    "            \n",
    "            sh_folder = os.path.join('scripts', TEST_PATH)\n",
    "            os.makedirs(sh_folder, exist_ok=True)\n",
    "            \n",
    "            scrpt = 'run5-'+method+'-'+ds+'-'+var+'.sh'\n",
    "            print('sh ' + scrpt)\n",
    "            \n",
    "            orig_stdout = sys.stdout\n",
    "            f = open(os.path.join(sh_folder, scrpt), 'w')\n",
    "            sys.stdout = f\n",
    "            print('#!/bin/bash')\n",
    "\n",
    "#             TESTS(data, results, prefix, var, descriptor, TOPRINT)\n",
    "            \n",
    "            json = json + '_' + var\n",
    "    \n",
    "            if method is 'MARC':\n",
    "                testesMARC(data, results, prefix, var, print_only)\n",
    "                \n",
    "            if method is 'POIFREQ':\n",
    "                testesPOIFREQ(k, data, results, prefix, var)\n",
    "                \n",
    "#           SUPER: testesSP(data, results, prefix, var, json, print_only)\n",
    "            if method is 'SMLD':\n",
    "                k_run(k, data, results, prefix, 'SMLD-'+var, json, Ms=-3, extra='-Al true', n_threads=THREADS, \n",
    "                prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', java_opts='-Xmx'+GIG+'G')\n",
    "            if method is 'SMD':\n",
    "                k_run(k, data, results, prefix, 'SMD-'+var, json, extra='-Al true', n_threads=THREADS, \n",
    "                prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', java_opts='-Xmx'+GIG+'G')\n",
    "            if method is 'SML':\n",
    "                k_run(k, data, results, prefix, 'SML-'+var, json, Ms=-3, n_threads=THREADS, \n",
    "                prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', java_opts='-Xmx'+GIG+'G')\n",
    "            if method is 'SM':\n",
    "                k_run(k, data, results, prefix, 'SM-'+var, json, n_threads=THREADS, \n",
    "                prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', java_opts='-Xmx'+GIG+'G')\n",
    "                \n",
    "#           MASTER: testesMM(data, results, prefix, var, json, print_only)\n",
    "            if method is 'MML':\n",
    "                k_run(k, data, results, prefix, 'MML-'+var, json, Ms=-3, n_threads=THREADS, \n",
    "                prg_path=prog_path, print_only=print_only, jar_name='MASTERMovelets', java_opts='-Xmx'+GIG+'G')\n",
    "            if method is 'MM':\n",
    "                k_run(k, data, results, prefix, 'MM-'+var,  json, n_threads=THREADS, \n",
    "                prg_path=prog_path, print_only=print_only, jar_name='MASTERMovelets', java_opts='-Xmx'+GIG+'G')\n",
    "                \n",
    "#           HIPER: testesM3(data, results, prefix, var, json, print_only)\n",
    "            if method is 'HL':\n",
    "                k_run(k, data, results, prefix,  'HL-'+var, json+'_hp', Ms=-3, extra='-version hiper -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "                prg_path=prog_path, print_only=print_only) #, java_opts='-Xmx'+GIG+'G')\n",
    "            if method is 'HpL':\n",
    "                k_run(k, data, results, prefix, 'HpL-'+var, json+'_hp', Ms=-3, extra='-version hiper-pvt -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "                prg_path=prog_path, print_only=print_only) #, java_opts='-Xmx'+GIG+'G')\n",
    "            if method is 'H':\n",
    "                k_run(k, data, results, prefix, 'H-'+var, json+'_hp', extra='-version hiper -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "                prg_path=prog_path, print_only=print_only) #, java_opts='-Xmx'+GIG+'G')\n",
    "            if method is 'Hp':\n",
    "                k_run(k, data, results, prefix, 'Hp-'+var, json+'_hp', extra='-version hiper-pvt -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "                prg_path=prog_path, print_only=print_only) #, java_opts='-Xmx'+GIG+'G')\n",
    "\n",
    "            print(\"# END - By Tarlis Portela\")\n",
    "            sys.stdout = orig_stdout\n",
    "            f.close()\n",
    "            \n",
    "    if method is not 'MARC' and method is not 'POIFREQ':\n",
    "        print('python3 automatize/acc4nn.py \"'+res_path+'\" \"'+TEST_PATH+'\"')\n",
    "    \n",
    "def scalability(method):\n",
    "    print('# SCALABILITY: ' + method)\n",
    "    print('Scalability-'+method+'.sh')\n",
    "    \n",
    "    THREADS = '4'#'3'\n",
    "    GIG = '60'\n",
    "    \n",
    "    orig_stdout = sys.stdout\n",
    "    f = open('scripts/Scalability/Scalability-'+method+'.sh','w')\n",
    "    sys.stdout = f\n",
    "    print('#!/bin/bash')\n",
    "    print('# --------------------------------------------------------------------------------------')\n",
    "    print('# - ETAPA 1 - Vary_Number_Of_Trajectories:                                             -')\n",
    "    print('# --------------------------------------------------------------------------------------')\n",
    "    prefixes    = [\n",
    "        '100_trajectories_50_points', \n",
    "        '500_trajectories_50_points', \n",
    "        '1000_trajectories_50_points', \n",
    "        '2000_trajectories_50_points', \n",
    "        '4000_trajectories_50_points'\n",
    "    ]\n",
    "    \n",
    "    for j in range(len(prefixes)):\n",
    "\n",
    "        variation   = 'Vary_Number_Of_Trajectories'\n",
    "        prefix      = prefixes[j]\n",
    "        descriptor  = os.path.join(data_folder, 'scalablity', 'descriptors', 'Scalability_1_Dimension')\n",
    "        results_dir = os.path.join(res_path, 'Scalablity', variation)\n",
    "        data    = os.path.join(data_folder, 'scalability', variation)\n",
    "\n",
    "        chooseScalRun(method, data, results_dir, prefix, prefix, descriptor, THREADS)\n",
    "        \n",
    "    print('# --------------------------------------------------------------------------------------')\n",
    "    print('# - ETAPA 2 - Vary_Number_of_Points:                                                   -')\n",
    "    print('# --------------------------------------------------------------------------------------')\n",
    "    prefixes    = [\n",
    "        '200_trajectories_10_points', \n",
    "        '200_trajectories_50_points', \n",
    "        '200_trajectories_100_points', \n",
    "        '200_trajectories_200_points', \n",
    "        '200_trajectories_400_points'\n",
    "    ]\n",
    "    \n",
    "    for j in range(len(prefixes)):\n",
    "\n",
    "        variation   = 'Vary_Number_of_Points'\n",
    "        prefix      = prefixes[j]\n",
    "        descriptor  = os.path.join(data_folder, 'scalablity','descriptors', 'Scalability_1_Dimension')\n",
    "        results_dir = os.path.join(res_path, 'Scalablity', variation)\n",
    "        data_dir    = os.path.join(data_folder, variation)\n",
    "\n",
    "        chooseScalRun(method, data, results_dir, prefix, prefix, descriptor, THREADS)\n",
    "\n",
    "        \n",
    "    print('# --------------------------------------------------------------------------------------')\n",
    "    print('# - ETAPA 3 - Vary_Number_of_Dimensions:                                               -')\n",
    "    print('# --------------------------------------------------------------------------------------')\n",
    "    prefixes    = [\n",
    "        'Scalability_1_Dimension', \n",
    "        'Scalability_2_Dimensions', \n",
    "        'Scalability_3_Dimensions',  \n",
    "        'Scalability_4_Dimensions', \n",
    "        'Scalability_5_Dimensions'\n",
    "    ]\n",
    "    for j in range(len(prefixes)):\n",
    "\n",
    "        variation   = 'Vary_Number_of_Dimensions'\n",
    "        prefix      = prefixes[j]\n",
    "        descriptor  = os.path.join(data_folder, 'scalablity', 'descriptors', prefix)\n",
    "        results_dir = os.path.join(res_path, 'Scalablity', 'Vary_Number_of_Dimensions')\n",
    "        data_dir    = os.path.join(data_folder, variation)\n",
    "\n",
    "        chooseScalRun(method, data, results_dir, prefix, prefix, descriptor, THREADS)\n",
    "        \n",
    "    sys.stdout = orig_stdout\n",
    "    f.close()\n",
    "    \n",
    "def chooseScalRun(method, data, results, prefix, var, json, THREADS):\n",
    "    print_only = True\n",
    "    javao = '-Xms13G -Xmx50G'\n",
    "    if method is 'SMLD':\n",
    "        run(data, results, prefix, 'SMLD-'+var, json, Ms=-3, extra='-Al true', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', keep_folder=0, java_opts=javao)\n",
    "    if method is 'SMD':\n",
    "        run(data, results, prefix, 'SMD-'+var, json, extra='-Al true', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', keep_folder=0, java_opts=javao)\n",
    "    if method is 'SML':\n",
    "        run(data, results, prefix, 'SML-'+var, json, Ms=-3, n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', keep_folder=0, java_opts=javao)\n",
    "    if method is 'SM':\n",
    "        run(data, results, prefix, 'SM-'+var, json, n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', keep_folder=0, java_opts=javao)\n",
    "\n",
    "#           MASTER: testesMM(data, results, prefix, var, json, print_only)\n",
    "    if method is 'MML':\n",
    "        run(data, results, prefix, 'MML-'+var, json, Ms=-3, n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='MASTERMovelets', keep_folder=0, java_opts=javao)\n",
    "    if method is 'MM':\n",
    "        run(data, results, prefix, 'MM-'+var,  json, n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='MASTERMovelets', keep_folder=0, java_opts=javao)\n",
    "\n",
    "#           HIPER: testesM3(data, results, prefix, var, json, print_only)\n",
    "    if method is 'HL':\n",
    "        run(data, results, prefix,  'HL-'+var, json+'_hp', Ms=-3, extra='-version hiper -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, keep_folder=0, java_opts=javao)\n",
    "    if method is 'HpL':\n",
    "        run(data, results, prefix, 'HpL-'+var, json+'_hp', Ms=-3, extra='-version hiper-pvt -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, keep_folder=0, java_opts=javao)\n",
    "    if method is 'H':\n",
    "        run(data, results, prefix, 'H-'+var, json+'_hp', extra='-version hiper -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, keep_folder=0, java_opts=javao)\n",
    "    if method is 'Hp':\n",
    "        run(data, results, prefix, 'Hp-'+var, json+'_hp', extra='-version hiper-pvt -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, keep_folder=0, java_opts=javao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalability(method):\n",
    "    print('REM SCALABILITY ' + method)\n",
    "    print('Scalability-'+method+'.bat')\n",
    "    \n",
    "    from io import StringIO\n",
    "    orig_stdout = sys.stdout\n",
    "    sys.stdout = txt = StringIO()\n",
    "    \n",
    "    scalability_var(method, 'Vary_Number_of_Trajectories', [100,500,1000,2000,4000], [50], [1])\n",
    "    scalability_var(method, 'Vary_Number_of_Points',       [200], [10,50,100,200,400], [1])\n",
    "    scalability_var(method, 'Vary_Number_of_Dimensions',   [200], [50], [1,2,3,4,5])\n",
    "       \n",
    "    txt = txt.getvalue()\n",
    "    txt = txt.replace('/','\\\\').replace('#', 'REM').replace('mkdir -p', 'md').replace('| tee -a', '>>').replace('rm -R', 'REM')\n",
    "    #.replace('# --------------------------------------------------------------------------------------','')\n",
    "        \n",
    "    sys.stdout = orig_stdout\n",
    "    f = open('scripts/Scalability/Scalability-'+method+'.bat.tarlis','w')\n",
    "    f.write(txt)\n",
    "    f.close()\n",
    "    \n",
    "def scalability_var(method, variation, trajs, points, dims):\n",
    "    \n",
    "    root = 'C:/Users\\\\05652761741\\\\Documents\\\\tarlis'\n",
    "    data_folder = (root+ '\\\\data')\n",
    "\n",
    "    res_path    = (root+ '\\\\results')\n",
    "    prog_path   = (root+ '\\\\programs')\n",
    "    \n",
    "    for t in trajs:\n",
    "        for p in points:\n",
    "            for d in dims:\n",
    "\n",
    "                prefix      = str(t)+'_trajs_'+str(p)+'_pts_'+str(d)+'_dim'\n",
    "                descriptor  = (data_folder+'\\\\descriptors'+'\\\\'+ 'Scalability_'+str(d)+'_Dimensions')\n",
    "                results_dir = (res_path+'\\\\'+ 'Scalablity')\n",
    "                if len(dims) == 1:\n",
    "                    data_dir    = (data_folder+'\\\\scalability\\\\'+variation+'\\\\'+str(t)+'_trajectories_'+str(p)+'_points')\n",
    "                else:\n",
    "                    data_dir    = (data_folder+'\\\\scalability\\\\Vary_Number_of_Dimensions')\n",
    "\n",
    "                chooseScalRun(method, data_dir, results_dir, variation, prefix, descriptor, 4)\n",
    "                \n",
    "def chooseScalRun(method, data, results, prefix, var, json, THREADS):\n",
    "    print_only = True\n",
    "    javao = '-Xms13G -Xmx50G'\n",
    "    prog_path = 'C:\\\\Users\\\\05652761741\\\\Documents\\\\tarlis\\\\programs'\n",
    "    if method is 'SMLD':\n",
    "        run(data, results, prefix, 'SMLD-'+var, json, Ms=-3, extra='-Al true', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', keep_folder=0, java_opts=javao)\n",
    "    if method is 'SMD':\n",
    "        run(data, results, prefix, 'SMD-'+var, json, extra='-Al true', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', keep_folder=0, java_opts=javao)\n",
    "    if method is 'SML':\n",
    "        run(data, results, prefix, 'SML-'+var, json, Ms=-3, n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', keep_folder=0, java_opts=javao)\n",
    "    if method is 'SM':\n",
    "        run(data, results, prefix, 'SM-'+var, json, n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='SUPERMovelets', keep_folder=0, java_opts=javao)\n",
    "\n",
    "#           MASTER: testesMM(data, results, prefix, var, json, print_only)\n",
    "    if method is 'MML':\n",
    "        run(data, results, prefix, 'MML-'+var, json, Ms=-3, n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='MASTERMovelets', keep_folder=0, java_opts=javao)\n",
    "    if method is 'MM':\n",
    "        run(data, results, prefix, 'MM-'+var,  json, n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, jar_name='MASTERMovelets', keep_folder=0, java_opts=javao)\n",
    "\n",
    "#           HIPER: testesM3(data, results, prefix, var, json, print_only)\n",
    "    if method is 'HL':\n",
    "        run(data, results, prefix,  'HL-'+var, json+'_hp', Ms=-3, extra='-version hiper -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, keep_folder=0, java_opts=javao)\n",
    "    if method is 'HpL':\n",
    "        run(data, results, prefix, 'HpL-'+var, json+'_hp', Ms=-3, extra='-version hiper-pvt -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, keep_folder=0, java_opts=javao)\n",
    "    if method is 'H':\n",
    "        run(data, results, prefix, 'H-'+var, json+'_hp', extra='-version hiper -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, keep_folder=0, java_opts=javao)\n",
    "    if method is 'Hp':\n",
    "        run(data, results, prefix, 'Hp-'+var, json+'_hp', extra='-version hiper-pvt -T 0.9 -BU 0.1', n_threads=THREADS, \n",
    "        prg_path=prog_path, print_only=print_only, keep_folder=0, java_opts=javao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalability('SMLD')\n",
    "scalability('SML')\n",
    "scalability('MML')\n",
    "scalability('HpL')\n",
    "scalability('HL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('# --------------------------------------------------------------------------------------')\n",
    "print('# - ETAPA 1:                                                                           -')\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "scripts('HpL', {'brightkite.Brightkite_Gowalla': ['specific']})\n",
    "scripts('HL', {'brightkite.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('Hp', {'brightkite.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('H', {'brightkite.Brightkite_Gowalla': ['specific']})\n",
    "\n",
    "scripts('HpL', {'gowalla.Brightkite_Gowalla': ['specific']})\n",
    "scripts('HL',  {'gowalla.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('Hp', {'gowalla.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('H',  {'gowalla.Brightkite_Gowalla': ['specific']})\n",
    "\n",
    "scripts('MML', {'gowalla.Brightkite_Gowalla': ['specific']})\n",
    "\n",
    "scripts('HpL', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "scripts('HL',  {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "# scripts('Hp',  {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "# scripts('H',   {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "\n",
    "# print('# --------------------------------------------------------------------------------------')\n",
    "# print('# - ESCALABILIDADE:                                                                    -')\n",
    "# print('# --------------------------------------------------------------------------------------')\n",
    "# scalability('HpL')\n",
    "# scalability('HL')\n",
    "# scalability('Hp')\n",
    "# scalability('H')\n",
    "\n",
    "# scalability('MML')\n",
    "\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "print('# - ETAPA 1.1:                                                                         -')\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "scripts('MML', {'foursquare_nyc.FoursquareNYC':  ['generic']})\n",
    "scripts('HL', {'foursquare_nyc.FoursquareNYC':  ['generic']})\n",
    "scripts('HpL', {'foursquare_nyc.FoursquareNYC':  ['generic']})\n",
    "# scripts('Hp', {'foursquare_nyc.FoursquareNYC':  ['generic']})\n",
    "# scripts('H', {'foursquare_nyc.FoursquareNYC':  ['generic']})\n",
    "\n",
    "scripts('MML', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "scripts('HpL', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "scripts('HL', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "# scripts('Hp', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "# scripts('H', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "\n",
    "scripts('HpL', {'grammatical_facial_expression.GrammaticalFaceExp':  ['specific']})\n",
    "scripts('HL', {'grammatical_facial_expression.GrammaticalFaceExp':  ['specific']})\n",
    "scripts('MML', {'grammatical_facial_expression.GrammaticalFaceExp':  ['specific']})\n",
    "# scripts('Hp', {'grammatical_facial_expression.GrammaticalFaceExp':  ['specific']})\n",
    "# scripts('H', {'grammatical_facial_expression.GrammaticalFaceExp':  ['specific']})\n",
    "\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "print('# - ETAPA 1.2 - GEO:                                                                   -')\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "scripts('HpL', {'brightkite.Brightkite_Gowalla': ['geo_only']})\n",
    "scripts('HpL',  {'gowalla.Brightkite_Gowalla': ['geo_only']})\n",
    "scripts('HpL',  {'foursquare_nyc.FoursquareNYC': ['geo_only']})\n",
    "scripts('HpL',   {'foursquare_global.FoursquareGlobal': ['geo_only']})\n",
    "\n",
    "scripts('HL', {'brightkite.Brightkite_Gowalla': ['geo_only']})\n",
    "scripts('HL',  {'gowalla.Brightkite_Gowalla': ['geo_only']})\n",
    "scripts('HL',  {'foursquare_nyc.FoursquareNYC': ['geo_only']})\n",
    "scripts('HL',   {'foursquare_global.FoursquareGlobal': ['geo_only']})\n",
    "\n",
    "# scripts('Hp', {'brightkite.Brightkite_Gowalla': ['geo_only']})\n",
    "# scripts('Hp',  {'gowalla.Brightkite_Gowalla': ['geo_only']})\n",
    "# scripts('Hp',  {'foursquare_nyc.FoursquareNYC': ['geo_only']})\n",
    "# scripts('Hp',   {'foursquare_global.FoursquareGlobal': ['geo_only']})\n",
    "\n",
    "# scripts('H', {'brightkite.Brightkite_Gowalla': ['geo_only']})\n",
    "# scripts('H',  {'gowalla.Brightkite_Gowalla': ['geo_only']})\n",
    "# scripts('H',  {'foursquare_nyc.FoursquareNYC': ['geo_only']})\n",
    "# scripts('H',   {'foursquare_global.FoursquareGlobal': ['geo_only']})\n",
    "\n",
    "scripts('MML', {'brightkite.Brightkite_Gowalla': ['geo_only']})\n",
    "scripts('MML',  {'gowalla.Brightkite_Gowalla': ['geo_only']})\n",
    "scripts('MML',  {'foursquare_nyc.FoursquareNYC': ['geo_only']})\n",
    "scripts('MML',   {'foursquare_global.FoursquareGlobal': ['geo_only']})\n",
    "\n",
    "scripts('POIFREQ', {'brightkite.Brightkite_Gowalla': ['space']})\n",
    "scripts('POIFREQ', {'gowalla.Brightkite_Gowalla': ['space']})\n",
    "scripts('POIFREQ', {'foursquare_nyc.FoursquareNYC': ['space']})\n",
    "scripts('POIFREQ', {'foursquare_global.FoursquareGlobal': ['space']})\n",
    "\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "print('# - ETAPA 1.3 - POI:                                                                   -')\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "scripts('HpL', {'brightkite.Brightkite_Gowalla': ['poi_only']})\n",
    "scripts('HpL', {'gowalla.Brightkite_Gowalla': ['poi_only']})\n",
    "scripts('HpL', {'foursquare_nyc.FoursquareNYC': ['poi_only']})\n",
    "scripts('HpL', {'foursquare_global.FoursquareGlobal': ['poi_only']})\n",
    "\n",
    "scripts('HL', {'brightkite.Brightkite_Gowalla': ['poi_only']})\n",
    "scripts('HL',  {'gowalla.Brightkite_Gowalla': ['poi_only']})\n",
    "scripts('HL',  {'foursquare_nyc.FoursquareNYC': ['poi_only']})\n",
    "scripts('HL',   {'foursquare_global.FoursquareGlobal': ['poi_only']})\n",
    "\n",
    "# scripts('Hp', {'brightkite.Brightkite_Gowalla': ['poi_only']})\n",
    "# scripts('Hp',  {'gowalla.Brightkite_Gowalla': ['poi_only']})\n",
    "# scripts('Hp',  {'foursquare_nyc.FoursquareNYC': ['poi_only']})\n",
    "# scripts('Hp',   {'foursquare_global.FoursquareGlobal': ['poi_only']})\n",
    "\n",
    "# scripts('H', {'brightkite.Brightkite_Gowalla': ['poi_only']})\n",
    "# scripts('H',  {'gowalla.Brightkite_Gowalla': ['poi_only']})\n",
    "# scripts('H',  {'foursquare_nyc.FoursquareNYC': ['poi_only']})\n",
    "# scripts('H',   {'foursquare_global.FoursquareGlobal': ['poi_only']})\n",
    "\n",
    "scripts('MML', {'brightkite.Brightkite_Gowalla': ['poi_only']})\n",
    "scripts('MML', {'gowalla.Brightkite_Gowalla': ['poi_only']})\n",
    "scripts('MML', {'foursquare_nyc.FoursquareNYC': ['poi_only']})\n",
    "scripts('MML', {'foursquare_global.FoursquareGlobal': ['poi_only']})\n",
    "\n",
    "scripts('POIFREQ', {'brightkite.Brightkite_Gowalla': ['poi']})\n",
    "scripts('POIFREQ', {'gowalla.Brightkite_Gowalla': ['poi']})\n",
    "scripts('POIFREQ', {'foursquare_nyc.FoursquareNYC': ['poi', 'category']})\n",
    "scripts('POIFREQ', {'foursquare_global.FoursquareGlobal': ['poi', 'category']})\n",
    "scripts('POIFREQ', {'promoters.GeneDS': ['sequence'], 'sjgs.GeneDS': ['sequence']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# --------------------------------------------------------------------------------------')\n",
    "print('# - ETAPA 2:                                                                           -')\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "# scripts('SMLD', {'brightkite.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('SML', {'brightkite.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('SMD', {'brightkite.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('SM', {'brightkite.Brightkite_Gowalla': ['specific']})\n",
    "\n",
    "scripts('SMLD', {'gowalla.Brightkite_Gowalla': ['specific']})\n",
    "scripts('SML', {'gowalla.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('SMD', {'gowalla.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('SM', {'gowalla.Brightkite_Gowalla': ['specific']})\n",
    "\n",
    "# scripts('MML', {'brightkite.Brightkite_Gowalla': ['specific']})\n",
    "\n",
    "scripts('SMLD', {'foursquare_nyc.FoursquareNYC':  ['generic']})\n",
    "scripts('SML', {'foursquare_nyc.FoursquareNYC':  ['generic']})\n",
    "# scripts('SMD', {'foursquare_nyc.FoursquareNYC':  ['generic']})\n",
    "# scripts('SM', {'foursquare_nyc.FoursquareNYC':  ['generic']})\n",
    "\n",
    "scripts('MML', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "\n",
    "scripts('SMLD', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "scripts('SML', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "print('# - ETAPA 2.1:                                                                         -')\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "scripts('SMLD', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "scripts('SML', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "# scripts('SMD', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "# scripts('SM', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "\n",
    "# scripts('SMD', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "# scripts('SM', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "print('# - ESCALABILIDADE:                                                                    -')\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "scalability('SMLD')\n",
    "scalability('SML')\n",
    "# scalability('SMD')\n",
    "# scalability('SM')\n",
    "\n",
    "# scalability('MM')\n",
    "\n",
    "# print('# --------------------------------------------------------------------------------------')\n",
    "# print('# - ETAPA 2.1:                                                                         -')\n",
    "# print('# --------------------------------------------------------------------------------------')\n",
    "# scripts('SMLD', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "# scripts('SML', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "# scripts('SMD', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "# scripts('SM', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "\n",
    "# scripts('SMLD', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "# scripts('SML', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "# scripts('SMD', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "# scripts('SM', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "print('# - ETAPA 2.2 - GEO:                                                                   -')\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "scripts('SMLD', {'brightkite.Brightkite_Gowalla': ['geo_only']})\n",
    "scripts('SMLD',  {'gowalla.Brightkite_Gowalla': ['geo_only']})\n",
    "scripts('SMLD',  {'foursquare_nyc.FoursquareNYC': ['geo_only']})\n",
    "scripts('SMLD',   {'foursquare_global.FoursquareGlobal': ['geo_only']})\n",
    "\n",
    "scripts('SML', {'brightkite.Brightkite_Gowalla': ['geo_only']})\n",
    "scripts('SML',  {'gowalla.Brightkite_Gowalla': ['geo_only']})\n",
    "scripts('SML',  {'foursquare_nyc.FoursquareNYC': ['geo_only']})\n",
    "scripts('SML',   {'foursquare_global.FoursquareGlobal': ['geo_only']})\n",
    "\n",
    "# scripts('SMD', {'brightkite.Brightkite_Gowalla': ['geo_only']})\n",
    "# scripts('SMD',  {'gowalla.Brightkite_Gowalla': ['geo_only']})\n",
    "# scripts('SMD',  {'foursquare_nyc.FoursquareNYC': ['geo_only']})\n",
    "# scripts('SMD',   {'foursquare_global.FoursquareGlobal': ['geo_only']})\n",
    "\n",
    "# scripts('SM', {'brightkite.Brightkite_Gowalla': ['geo_only']})\n",
    "# scripts('SM',  {'gowalla.Brightkite_Gowalla': ['geo_only']})\n",
    "# scripts('SM',  {'foursquare_nyc.FoursquareNYC': ['geo_only']})\n",
    "# scripts('SM',   {'foursquare_global.FoursquareGlobal': ['geo_only']})\n",
    "\n",
    "\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "print('# - ETAPA 2.3 - POI:                                                                   -')\n",
    "print('# --------------------------------------------------------------------------------------')\n",
    "scripts('SMLD', {'brightkite.Brightkite_Gowalla': ['poi_only']})\n",
    "scripts('SMLD',  {'gowalla.Brightkite_Gowalla': ['poi_only']})\n",
    "scripts('SMLD',  {'foursquare_nyc.FoursquareNYC': ['poi_only']})\n",
    "scripts('SMLD',   {'foursquare_global.FoursquareGlobal': ['poi_only']})\n",
    "\n",
    "scripts('SML', {'brightkite.Brightkite_Gowalla': ['poi_only']})\n",
    "scripts('SML',  {'gowalla.Brightkite_Gowalla': ['poi_only']})\n",
    "scripts('SML',  {'foursquare_nyc.FoursquareNYC': ['poi_only']})\n",
    "scripts('SML',   {'foursquare_global.FoursquareGlobal': ['poi_only']})\n",
    "\n",
    "# scripts('SMD', {'brightkite.Brightkite_Gowalla': ['poi_only']})\n",
    "# scripts('SMD',  {'gowalla.Brightkite_Gowalla': ['poi_only']})\n",
    "# scripts('SMD',  {'foursquare_nyc.FoursquareNYC': ['poi_only']})\n",
    "# scripts('SMD',   {'foursquare_global.FoursquareGlobal': ['poi_only']})\n",
    "\n",
    "# scripts('SM', {'brightkite.Brightkite_Gowalla': ['poi_only']})\n",
    "# scripts('SM',  {'gowalla.Brightkite_Gowalla': ['poi_only']})\n",
    "# scripts('SM',  {'foursquare_nyc.FoursquareNYC': ['poi_only']})\n",
    "# scripts('SM',   {'foursquare_global.FoursquareGlobal': ['poi_only']})\n",
    "\n",
    "\n",
    "# print('# --------------------------------------------------------------------------------------')\n",
    "# print('# - ETAPA 3 - OUTROS:                                                                  -')\n",
    "# print('# --------------------------------------------------------------------------------------')\n",
    "# scripts('MM', {'promoters.GeneDS': ['specific'], 'sjgs.GeneDS': ['specific']})\n",
    "\n",
    "# scripts('MM', {'brightkite.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('MM', {'foursquare_nyc.FoursquareNYC':  ['generic']})\n",
    "# scripts('MM', {'gowalla.Brightkite_Gowalla': ['specific']})\n",
    "# scripts('MM', {'foursquare_nyc.FoursquareNYC':  ['specific']})\n",
    "# # scripts('MM', {'foursquare_global.FoursquareGlobal': ['specific']})\n",
    "\n",
    "# scripts('MM', {'brightkite.Brightkite_Gowalla': ['geo_only']})\n",
    "# scripts('MM', {'gowalla.Brightkite_Gowalla': ['geo_only']})\n",
    "# scripts('MM', {'foursquare_nyc.FoursquareNYC': ['geo_only']})\n",
    "# scripts('MM', {'foursquare_global.FoursquareGlobal': ['geo_only']})\n",
    "\n",
    "# scripts('MM', {'brightkite.Brightkite_Gowalla': ['poi_only']})\n",
    "# scripts('MM', {'gowalla.Brightkite_Gowalla': ['poi_only']})\n",
    "# scripts('MM', {'foursquare_nyc.FoursquareNYC': ['poi_only']})\n",
    "# scripts('MM', {'foursquare_global.FoursquareGlobal': ['poi_only']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method      = 'POI-FREQ'\n",
    "datasets    = {'brightkite.Brightkite_Gowalla': ['poi', 'space'], \n",
    "               'gowalla.Brightkite_Gowalla': ['poi', 'space'], \n",
    "               'foursquare_nyc.FoursquareNYC': ['poi', 'category', 'space'], \n",
    "               'foursquare_global.FoursquareGlobal': ['poi', 'category', 'space']}\n",
    "\n",
    "scripts('POIFREQ', datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method      = 'hiper'\n",
    "datasets    = {'brightkite.Brightkite_Gowalla': ['poi_only', 'geo_only', 'specific'], \n",
    "               'gowalla.Brightkite_Gowalla': ['poi_only', 'geo_only', 'specific'], \n",
    "               'foursquare_nyc.FoursquareNYC': ['poi_only', 'geo_only', 'generic', 'specific'], \n",
    "               'foursquare_global.FoursquareGlobal': ['poi_only', 'geo_only', 'generic', 'specific']}\n",
    "\n",
    "scripts('HpL', datasets)\n",
    "scripts('HL', datasets)\n",
    "scripts('Hp', datasets)\n",
    "scripts('H', datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets    = {'promoters.GeneDS': ['specific'], 'splice-junction-gene-sequences.GeneDS': ['specific']}\n",
    "\n",
    "scripts('HpL', datasets)\n",
    "scripts('MML', datasets)\n",
    "scripts('MARC', datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method      = 'super'\n",
    "datasets    = {'brightkite.Brightkite_Gowalla': ['poi_only', 'geo_only', 'specific'], \n",
    "               'gowalla.Brightkite_Gowalla': ['poi_only', 'geo_only', 'specific'], \n",
    "               'foursquare_nyc.FoursquareNYC': ['poi_only', 'geo_only', 'generic', 'specific'], \n",
    "               'foursquare_global.FoursquareGlobal': ['poi_only', 'geo_only', 'generic', 'specific']}\n",
    "\n",
    "scripts('SMLD', datasets)\n",
    "scripts('SMD', datasets)\n",
    "scripts('SML', datasets)\n",
    "scripts('SM', datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method      = 'master'\n",
    "datasets    = {'brightkite.Brightkite_Gowalla': ['poi_only', 'geo_only', 'specific'], \n",
    "               'gowalla.Brightkite_Gowalla': ['poi_only', 'geo_only', 'specific'], \n",
    "               'foursquare_nyc.FoursquareNYC': ['poi_only', 'geo_only', 'generic', 'specific'], \n",
    "               'foursquare_global.FoursquareGlobal': ['poi_only', 'geo_only', 'generic', 'specific']}\n",
    "\n",
    "scripts('MML', datasets)\n",
    "scripts('MM', datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method      = 'MARC'\n",
    "datasets    = {'brightkite.Brightkite_Gowalla': ['poi_only', 'geo_only', 'specific'], \n",
    "               'gowalla.Brightkite_Gowalla': ['poi_only', 'geo_only', 'specific'], \n",
    "               'foursquare_nyc.FoursquareNYC': ['poi_only', 'geo_only', 'generic', 'specific'], \n",
    "               'foursquare_global.FoursquareGlobal': ['poi_only', 'geo_only', 'generic', 'specific']}\n",
    "\n",
    "scripts('MARC', datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BrightkiteTOPRINT = True\n",
    "print('#!/bin/bash')\n",
    "\n",
    "TEST_PATH = '3 - Sinais Sandra'\n",
    "# -----------------------------\n",
    "\n",
    "DESC_NAME = 'Sinais_ALL'\n",
    "data = os.path.join(data_folder, 'Sinais/SC_DM1')\n",
    "prefix = os.path.join(TEST_PATH, 'Sinais')\n",
    "\n",
    "TESTS(data, res_path, prefix, '', DESC_NAME, TOPRINT)\n",
    "\n",
    "\n",
    "TEST_PATH = '4 - DS FR'\n",
    "# -----------------------------\n",
    "\n",
    "DESC_NAME = 'ECG_ALL'\n",
    "data = os.path.join(data_folder, 'autoCEP_DS/ECG_converted')\n",
    "prefix = os.path.join(TEST_PATH, 'ECG-testendo')\n",
    "\n",
    "TESTS(data, res_path, prefix, '', DESC_NAME, TOPRINT)\n",
    "\n",
    "DESC_NAME = 'Wafer_ALL'\n",
    "data = os.path.join(data_folder, 'autoCEP_DS/Wafer_converted')\n",
    "prefix = os.path.join(TEST_PATH, 'Wafer')\n",
    "\n",
    "TESTS(data, res_path, prefix, '', DESC_NAME, TOPRINT)\n",
    "\n",
    "DESC_NAME = 'AR_ALL'\n",
    "data = os.path.join(data_folder, 'autoCEP_DS/AR_converted')\n",
    "prefix = os.path.join(TEST_PATH, 'Activity Recognition')\n",
    "\n",
    "TESTS(data, res_path, prefix, '', DESC_NAME, TOPRINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatize.analysis import def_random_seed, results2df, ACC4All, ALL3, MLP, printLatex, loadData, RN4All\n",
    "def_random_seed(random_num=1, seed_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python automatize/acc4all.py '/Users/tarlis/Documents/results' 'FNY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Fnyc-2'\n",
    "results     = os.path.join(res_path, 'test-192')\n",
    "\n",
    "RN4All(results, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results2df(results, prefix)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results2df(results, 'Foursquare NYC')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatize.preprocessing import kfold_trainAndTestSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(cols, dataset, i):\n",
    "    folder = os.path.join(data_folder, '5fold', dataset, 'run'+str(i))\n",
    "    \n",
    "    train = zip2csv(folder, 'train', cols, class_col = 'label')\n",
    "    test = zip2csv(folder, 'test', cols, class_col = 'label')\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "def saveas(data, cols, dataset, i, file,  spacecol=None):\n",
    "    if spacecol is not None:\n",
    "        # new data frame with split value columns \n",
    "        ll = data[spacecol].str.split(\" \", n = 1, expand = True) \n",
    "        data[\"lat\"]= ll[0]\n",
    "        data[\"lon\"]= ll[1]\n",
    "        data.drop(columns =[spacecol], inplace = True) \n",
    "        \n",
    "    df = data[cols]\n",
    "    \n",
    "    folder = os.path.join(data_folder, '5fold', dataset, 'run'+str(i))\n",
    "    df.to_csv(os.path.join(folder, file+'.csv'), index = False)    \n",
    "        \n",
    "\n",
    "# dataset = 'brightkite' \n",
    "# dataset = 'gowalla'   \n",
    "dataset = 'foursquare_ny'      \n",
    "\n",
    "for i in range(1, 6):\n",
    "    cols   = ['space', 'datetime', 'time', 'day', 'category', 'price', 'rating', 'weather', 'category_number']\n",
    "    x,y = convert(cols, dataset, i)\n",
    "    \n",
    "    orderc = ['tid', 'label', 'lat', 'lon', 'datetime', 'time', 'day', 'category', 'category_number', 'price', 'rating', 'weather']\n",
    "    saveas(x, orderc, dataset, i, 'all'+'_train', 'space')\n",
    "    saveas(y, orderc, dataset, i, 'all'+'_test',  'space')\n",
    "    \n",
    "    orderc = ['tid', 'label', 'lat', 'lon', 'time', 'day', 'category', 'price', 'rating', 'weather']\n",
    "    saveas(x, orderc, dataset, i, 'generic'+'_train', None)\n",
    "    saveas(y, orderc, dataset, i, 'generic'+'_test',  None)\n",
    "    \n",
    "    orderc = ['tid', 'label', 'lat', 'lon']\n",
    "    saveas(x, orderc, dataset, i, 'geo_only'+'_train', None)\n",
    "    saveas(y, orderc, dataset, i, 'geo_only'+'_test',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'foursquare_nyc'  \n",
    "# dataset = 'foursquare_global'  \n",
    "\n",
    "dir_path = os.path.join(data_folder, '5fold', dataset)\n",
    "cols = ['tid','label','lat','lon','day','hour','poi','category','price','rating','weather']\n",
    "\n",
    "data = joinTrainAndTest(dir_path, cols, train_file=\"specific_train.csv\", test_file=\"specific_test.csv\", class_col = 'label')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "# df = data[['tid','label','lat','lon','day','hour','poi','category','price','rating','weather']]\n",
    "train, test = kfold_trainAndTestSplit(dir_path, k, data, random_num=1, class_col='label', fileprefix='specific_', columns_order=cols)\n",
    "\n",
    "cols = ['tid','label','day','hour','category','price','rating','weather']\n",
    "kfold_trainAndTestSplit(dir_path, k, None, random_num=1, class_col='label', fileprefix='generic_', columns_order=cols, ktrain=train, ktest=test)\n",
    "\n",
    "cols = ['tid','label','lat','lon']\n",
    "kfold_trainAndTestSplit(dir_path, k, None, random_num=1, class_col='label', fileprefix='geo_only_', columns_order=cols, ktrain=train, ktest=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from _dummy_thread import exit\n",
    "\n",
    "from datetime import datetime\n",
    "# --------------------------------------------------------------------------------------\n",
    "def ApproachLSTM(X_train, y_train, X_test, y_test, par_batch_size, lst_par_epochs, lst_par_lr, par_dropout, save_results, dir_path, modelfolder='model') :\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, LSTM\n",
    "    from keras.optimizers import Adam\n",
    "    import pandas as pd\n",
    "    import os\n",
    "        \n",
    "    nattr = len(X_train[1,:])    \n",
    "\n",
    "    # Scaling y and transforming to keras format\n",
    "    from sklearn import preprocessing\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train = le.transform(y_train) \n",
    "    y_test = le.transform(y_test)\n",
    "    from keras.utils import to_categorical\n",
    "    y_train1 = to_categorical(y_train)\n",
    "    y_test1 = to_categorical(y_test)\n",
    "    nclasses = len(le.classes_)\n",
    "    \n",
    "    units = 100\n",
    "    \n",
    "    #Initializing Neural Network\n",
    "    model = Sequential()\n",
    "    # Adding the LSTM Layer\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    # Adding the input layer and the first hidden layer\n",
    "#     model.add(Dense(units = units, kernel_initializer = 'uniform', activation = 'linear', input_dim = (nattr)))\n",
    "    model.add(Dropout( par_dropout ))\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(units = nclasses, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "    # Compiling Neural Network\n",
    "    \n",
    "    k = len(lst_par_epochs)\n",
    "    \n",
    "    for k in range(0,k) :\n",
    "           \n",
    "        adam = Adam(lr=lst_par_lr[k])\n",
    "        model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy','top_k_categorical_accuracy',f1])\n",
    "        history = model.fit(X_train, y_train1, validation_data = (X_test, y_test1), epochs=lst_par_epochs[k], batch_size=par_batch_size)\n",
    "    \n",
    "        # ---------------------------------------------------------------------------------\n",
    "        if (save_results) :\n",
    "            if not os.path.exists(os.path.join(dir_path, modelfolder)):\n",
    "                os.makedirs(os.path.join(dir_path, modelfolder))\n",
    "            model.save(os.path.join(dir_path, modelfolder, 'model_approach2_Step'+str(k+1)+'.h5'))\n",
    "            from numpy import argmax\n",
    "            y_test_true_dec = le.inverse_transform(argmax(y_test1, axis = 1))\n",
    "            y_test_pred_dec =  le.inverse_transform(argmax( model.predict(X_test) , axis = 1)) \n",
    "            report = classification_report(y_test_true_dec, y_test_pred_dec )\n",
    "            classification_report_csv(report, os.path.join(dir_path, modelfolder, 'model_approach2_report_Step'+str(k+1)+'.csv'), \"Approach2_Step\"+str(k+1)) \n",
    "            pd.DataFrame(history.history).to_csv(os.path.join(dir_path, modelfolder, 'model_approach2_history_Step'+str(k+1)+'.csv'))\n",
    "            pd.DataFrame(y_test_true_dec,y_test_pred_dec).to_csv(os.path.join(dir_path, modelfolder, 'model_approach2_prediction_Step'+str(k+1)+'.csv'), header = True)  \n",
    "            \n",
    "# ----------------------------------------------------------------------------------\n",
    "def Classifier_LSTM(dir_path, save_results = True, modelfolder='model', X_train = None, y_train = None, X_test = None, y_test = None):\n",
    "    if X_train is None:\n",
    "        X_train, y_train, X_test, y_test = loadData(dir_path)\n",
    "     \n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Neural Network - Definitions:\n",
    "    par_droupout = 0.5\n",
    "    par_batch_size = 200\n",
    "    par_epochs = 80\n",
    "    par_lr = 0.00095\n",
    "    \n",
    "    # Building the neural network-\n",
    "    print(\"Building neural network\")\n",
    "    lst_par_epochs = [80,50,50,30,20]\n",
    "    lst_par_lr = [0.00095,0.00075,0.00055,0.00025,0.00015]\n",
    "    \n",
    "    time = datetime.now()\n",
    "    ApproachLSTM(X_train, y_train, X_test, y_test, par_batch_size, lst_par_epochs, lst_par_lr, par_droupout, save_results, dir_path, modelfolder)\n",
    "    time = (datetime.now()-time).total_seconds() * 1000\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    print(\"Done. \" + str(time) + \" milliseconds\")\n",
    "    print(\"---------------------------------------------------------------------------------\")\n",
    "    return time\n",
    "\n",
    "Classifier_LSTM(os.path.join(res_path, prefix, 'HpL-T50-geo_only'), modelfolder='model_lstm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
