{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos para Paper Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root     = './../'\n",
    "data     = os.path.join(root, 'new_data')\n",
    "res_path = os.path.join(root, 'results')\n",
    "prg_path = os.path.join(root, 'programs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatize.ensemble import ApproachEnsemble\n",
    "from automatize.ensemble_models.poifreq_model import poifreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'run1'\n",
    "prefix = run+'/Promoters'\n",
    "results     = os.path.join(res_path, 'HpL-5fold_4T_60G')\n",
    "folder      = os.path.join(data, 'promoters', run)\n",
    "\n",
    "results_dir = os.path.join(res_path, 'teste_poifreq', 'teste_ensemble')\n",
    "\n",
    "sequences = [2, 3]\n",
    "features  = ['sequence']\n",
    "agg_x_train, agg_x_test, y_train, y_test, core_name = poifreq(sequences, '', features, folder, results_dir, save_all=True)\n",
    "\n",
    "ApproachEnsemble(os.path.join(results, prefix, 'HpL-specific'), core_name, save_results=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatize.preprocessing import kfold_trainAndTestSplit, printFeaturesJSON\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets - ?\n",
    " - A.R.: http://archive.ics.uci.edu/ml/datasets/Activity+Recognition+system+based+on+Multisensor+data+fusion+%28AReM%29\n",
    " - Sign Language: http://archive.ics.uci.edu/ml/datasets/Activity+Recognition+system+based+on+Multisensor+data+fusion+%28AReM%29\n",
    " - Geo-Magnetic field and WLAN dataset for indoor localisation from wristband and smartphone Data Set (http://archive.ics.uci.edu/ml/datasets/Geo-Magnetic+field+and+WLAN+dataset+for+indoor+localisation+from+wristband+and+smartphone)\n",
    " - Activities of Daily Living (ADLs) Recognition Using Binary Sensors Data Set (http://archive.ics.uci.edu/ml/datasets/Activities+of+Daily+Living+%28ADLs%29+Recognition+Using+Binary+Sensors)\n",
    " - Hill-Valley Data Set https://archive.ics.uci.edu/ml/datasets/Hill-Valley\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer\n",
    "# OUTRO Maior: http://archive.ics.uci.edu/ml/datasets/Activity+Recognition+system+based+on+Multisensor+data+fusion+%28AReM%29\n",
    "dir_path = os.path.join(data, 'Activity Recognition')\n",
    "file = '1.csv'\n",
    "\n",
    "cols = ['seq','x','y','z','label']\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, file), names=cols)\n",
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://archive.ics.uci.edu/ml/datasets/clickstream+data+for+online+shopping\n",
    "# Interessante pra classificar o padrão de compra por país\n",
    "dir_path = os.path.join(data, 'eshop')\n",
    "file = 'e-shop clothing 2008.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, file), sep=';')\n",
    "df.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Promoter+Gene+Sequences%29\n",
    "dir_path = os.path.join(data, 'protein')\n",
    "file = 'promoters.data'\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, file), sep=',', names=['label', 'name', 'sequence'])\n",
    "df['sequence'] = df['sequence'].replace(\"\\t\", '', regex=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29\n",
    "dir_path = os.path.join(data, 'splice-junction-gene-sequences')\n",
    "file = 'splice.data'\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, file), sep=',', names=['label', 'name', 'sequence'])\n",
    "# df['sequence'] = df['sequence'].replace(\"\\t\", '', regex=True)\n",
    "# df = df[['label', 'sequence']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df[0]['sequence'].str)\n",
    "ds = pd.DataFrame()\n",
    "tid = 1\n",
    "for row in df.iterrows():\n",
    "    aux = pd.DataFrame()\n",
    "    aux['sequence'] = list(row[1]['sequence'].strip())\n",
    "    aux['name'] = row[1]['name'].strip()\n",
    "    aux['label'] = row[1]['label'].strip()\n",
    "    aux['tid'] = tid\n",
    "    tid = tid + 1\n",
    "    ds = pd.concat([ds, aux])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_csv(os.path.join(dir_path, 'promoters_data.csv'), index = False) \n",
    "train, test = kfold_trainAndTestSplit(dir_path, 5, ds, random_num=1, class_col='label', \\\n",
    "                                      fileprefix='', columns_order=['tid','sequence', 'name', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zenodo.org/record/3884398#.X0bNJtNKjiw\n",
    "dir_path = os.path.join(data, 'GECCO_Water')\n",
    "file = '1_gecco2018_water_quality.csv'\n",
    "\n",
    "cols = [\"id\",\"Time\",\"Tp\",\"Cl\",\"pH\",\"Redox\",\"Leit\",\"Trueb\",\"Cl_2\",\"Fm\",\"Fm_2\",\"EVENT\"]\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, file))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.covidactnow.org/export-covid-act-now-data-spreadsheet/\n",
    "dir_path = os.path.join(data, 'covid_act')\n",
    "\n",
    "cols = ['date', 'hospitalBedsRequired', 'hospitalBedCapacity', 'ICUBedsInUse',\n",
    "       'ICUBedCapacity', 'ventilatorsInUse', 'ventilatorCapacity',\n",
    "       'RtIndicator', 'RtIndicatorCI90', 'cumulativeDeaths',\n",
    "       'cumulativeInfected', 'currentInfected', 'currentSusceptible',\n",
    "       'currentExposed', 'countryName', 'stateName', 'countyName',\n",
    "       'intervention', 'fips', 'lat', 'long', 'lastUpdatedDate']\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "df1 = pd.read_csv(os.path.join(dir_path, 'states.NO_INTERVENTION.timeseries.csv'))\n",
    "# df['class'] = 'NO_INTERVENTION'\n",
    "# dataset = pd.concat([dataset, df])\n",
    "\n",
    "# df = pd.read_csv(os.path.join(dir_path, 'states.WEAK_INTERVENTION.timeseries.csv'))\n",
    "# df['class'] = 'WEAK_INTERVENTION'\n",
    "# dataset = pd.concat([dataset, df])\n",
    "\n",
    "df2 = pd.read_csv(os.path.join(dir_path, 'states.STRONG_INTERVENTION.timeseries.csv'))\n",
    "df['class'] = 'STRONG_INTERVENTION'\n",
    "dataset = pd.concat([dataset, df])\n",
    "# # df.hospitalBedsRequired.unique()\n",
    "dataset.stateName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df1.merge(df2, indicator=True, how='outer')\n",
    "merged[merged['_merge'] == 'right_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.countyName.unique()\n",
    "# dataset.columns\n",
    "dataset.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.date.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['date', 'hospitalBedsRequired', 'hospitalBedCapacity', 'ICUBedsInUse',\n",
    "       'ICUBedCapacity', 'ventilatorsInUse', 'ventilatorCapacity',\n",
    "       'RtIndicator', 'RtIndicatorCI90', 'cumulativeDeaths',\n",
    "       'cumulativeInfected', 'currentInfected', 'currentSusceptible',\n",
    "       'currentExposed', 'stateName', 'countryName', 'fips', 'label']\n",
    "\n",
    "dataset.to_csv(os.path.join(dir_path, 'covid_act.csv'), index = False) \n",
    "train, test = kfold_trainAndTestSplit(dir_path, 5, dataset, random_num=1, class_col='label', \\\n",
    "                                      fileprefix='', columns_order=['tid','sequence', 'name', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://archive.ics.uci.edu/ml/datasets/Grammatical+Facial+Expressions\n",
    "dir_path = os.path.join(data, 'grammatical_facial_expression')\n",
    "\n",
    "import glob\n",
    "f = glob.glob(os.path.join(dir_path, '*_datapoints.txt'))\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "tid = 1\n",
    "for file in f:\n",
    "    name = os.path.basename(os.path.normpath(file))[:-15].split('_',1)\n",
    "    df1 = pd.read_csv(os.path.join(dir_path, name[0]+'_'+name[1]+'_datapoints.txt'), sep=' ')\n",
    "    df2 = pd.read_csv(os.path.join(dir_path, name[0]+'_'+name[1]+'_targets.txt'), names=['target'], header=None)\n",
    "    df1 = pd.concat([df1, df2], axis=1)\n",
    "    \n",
    "    df1['user']  = name[0]\n",
    "    df1['label'] = name[1]\n",
    "    df1['tid']   = 0\n",
    "    \n",
    "    for index, row in df1.iterrows():\n",
    "        if df1.loc[index,'target'] == 0:\n",
    "            df1.loc[index,'label'] = 'none'\n",
    "                        \n",
    "        df1.loc[index,'tid'] = tid\n",
    "        \n",
    "        if index < df1.shape[0]-1 and df1.loc[index+1,'target'] != df1.loc[index,'target']:\n",
    "            tid = tid+1\n",
    "    \n",
    "    dataset = pd.concat([dataset, df1])\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.reset_index(drop=True)\n",
    "# list(dataset.columns)[:305]\n",
    "# dataset\n",
    "cols = cols = ['tid', '0.0', '0x', '0y', '0z', '1x', '1y', '1z', '2x', '2y', '2z', '3x', \\\n",
    "'3y', '3z', '4x', '4y', '4z', '5x', '5y', '5z', '6x', '6y', '6z', '7x', '7y', \\\n",
    "'7z', '8x', '8y', '8z', '9x', '9y', '9z', '10x', '10y', '10z', '11x', '11y', \\\n",
    "'11z', '12x', '12y', '12z', '13x', '13y', '13z', '14x', '14y', '14z', '15x', \\\n",
    "'15y', '15z', '16x', '16y', '16z', '17x', '17y', '17z', '18x', '18y', '18z', \\\n",
    "'19x', '19y', '19z', '20x', '20y', '20z', '21x', '21y', '21z', '22x', '22y', \\\n",
    "'22z', '23x', '23y', '23z', '24x', '24y', '24z', '25x', '25y', '25z', '26x', \\\n",
    "'26y', '26z', '27x', '27y', '27z', '28x', '28y', '28z', '29x', '29y', '29z', \\\n",
    "'30x', '30y', '30z', '31x', '31y', '31z', '32x', '32y', '32z', '33x', '33y', \\\n",
    "'33z', '34x', '34y', '34z', '35x', '35y', '35z', '36x', '36y', '36z', '37x', \\\n",
    "'37y', '37z', '38x', '38y', '38z', '39x', '39y', '39z', '40x', '40y', '40z', \\\n",
    "'41x', '41y', '41z', '42x', '42y', '42z', '43x', '43y', '43z', '44x', '44y', \\\n",
    "'44z', '45x', '45y', '45z', '46x', '46y', '46z', '47x', '47y', '47z', '48x', \\\n",
    "'48y', '48z', '49x', '49y', '49z', '50x', '50y', '50z', '51x', '51y', '51z', \\\n",
    "'52x', '52y', '52z', '53x', '53y', '53z', '54x', '54y', '54z', '55x', '55y', \\\n",
    "'55z', '56x', '56y', '56z', '57x', '57y', '57z', '58x', '58y', '58z', '59x', \\\n",
    "'59y', '59z', '60x', '60y', '60z', '61x', '61y', '61z', '62x', '62y', '62z', \\\n",
    "'63x', '63y', '63z', '64x', '64y', '64z', '65x', '65y', '65z', '66x', '66y', \\\n",
    "'66z', '67x', '67y', '67z', '68x', '68y', '68z', '69x', '69y', '69z', '70x', \\\n",
    "'70y', '70z', '71x', '71y', '71z', '72x', '72y', '72z', '73x', '73y', '73z', \\\n",
    "'74x', '74y', '74z', '75x', '75y', '75z', '76x', '76y', '76z', '77x', '77y', \\\n",
    "'77z', '78x', '78y', '78z', '79x', '79y', '79z', '80x', '80y', '80z', '81x', \\\n",
    "'81y', '81z', '82x', '82y', '82z', '83x', '83y', '83z', '84x', '84y', '84z', \\\n",
    "'85x', '85y', '85z', '86x', '86y', '86z', '87x', '87y', '87z', '88x', '88y', \\\n",
    "'88z', '89x', '89y', '89z', '90x', '90y', '90z', '91x', '91y', '91z', '92x', \\\n",
    "'92y', '92z', '93x', '93y', '93z', '94x', '94y', '94z', '95x', '95y', '95z', \\\n",
    "'96x', '96y', '96z', '97x', '97y', '97z', '98x', '98y', '98z', '99x', '99y', \\\n",
    "'99z', 'label']\n",
    "\n",
    "# printFeaturesJSON(cols, 2, 'numeric', 'difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold train and test split in... ./../new_data/grammatical_facial_expression\n",
      "Reading files...\n",
      "Done.\n",
      "Writing files...\n",
      "Done.\n",
      " --------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset.to_csv(os.path.join(dir_path, 'grammatical_facial_expression_data.csv'), index = False) \n",
    "\n",
    "# Remove os Nones:\n",
    "dataset = dataset[dataset.label != 'none']\n",
    "\n",
    "train, test = kfold_trainAndTestSplit(dir_path, 5, dataset, random_num=1, class_col='label', \\\n",
    "                                      fileprefix='specific_', columns_order=cols)\n",
    "\n",
    "cols.remove('label')\n",
    "train, test = kfold_trainAndTestSplit(dir_path, 5, dataset, random_num=1, class_col='label', \\\n",
    "                                      fileprefix='', columns_order=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatize.analysis import def_random_seed, results2df, MLP, printLatex, loadData, kFoldResults\n",
    "def_random_seed(random_num=1, seed_num=1)\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'run1/Promoters'\n",
    "results     = os.path.join(res_path, 'HpL-5fold_4T_60G')\n",
    "\n",
    "# RN4All(results, prefix)\n",
    "ApproachEnsemble(os.path.join(results, prefix, 'HpL-specific'), modelfolder='model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'run4/Foursquare NYC'\n",
    "results     = os.path.join(res_path, 'hiper-compare_v2')\n",
    "\n",
    "# RN4All(results, prefix)\n",
    "# ApproachEnsemble(os.path.join(results, prefix, 'HpL-generic'), save_results=False)\n",
    "ApproachEnsemble(os.path.join(data, 'promoters'), save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = results2df(results, prefix)\n",
    "df = kFoldResults(results, 'Promoters', 'HpL-specific')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApproachEnsemble(dir_path, dir_path2, save_results=True, modelfolder='model'):\n",
    "        \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow\n",
    "        \n",
    "    X_train, y_train, X_test, y_test = loadData(dir_path)\n",
    "    keys, vocab_size, num_classes, max_length, X_train2, y_train2, X_test2, y_test2 = get_trajectories(dir_path2)  \n",
    "        \n",
    "    labels   = list(set(y_train))\n",
    "    y_train  = [labels.index(x) for x in y_train]\n",
    "    y_test   = [labels.index(x) for x in y_test]\n",
    "#     y_train2 = [labels.index(x) for x in y_train2]\n",
    "#     y_test2  = [labels.index(x) for x in y_test2]\n",
    "    \n",
    "    print(\"Building Ensemble models\")\n",
    "    time = datetime.now()\n",
    "    \n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    pred1 = model_movelets(X_train, y_train, X_test, y_test)\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    pred2 = model_marc(keys, vocab_size, num_classes, max_length, X_train2, y_train2, X_test2, y_test2)\n",
    "\n",
    "    y_pred1 = [np.argmax(f) for f in pred1]\n",
    "    y_pred2 = [np.argmax(f) for f in pred2]\n",
    "\n",
    "    final_pred = (pred1*0.5+pred2*0.5)\n",
    "    y_pred = [np.argmax(f) for f in final_pred]\n",
    "    \n",
    "    print('Models results:')\n",
    "    print(get_line(y_test, y_pred1))\n",
    "    print(get_line(y_test, y_pred2))\n",
    "    \n",
    "    print('Ensembled results:')\n",
    "    line=get_line(y_test, y_pred)\n",
    "    print(line)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------\n",
    "    if (save_results) :\n",
    "        if not os.path.exists(os.path.join(dir_path, modelfolder)):\n",
    "            os.makedirs(os.path.join(dir_path, modelfolder))\n",
    "        report = classification_report(y_test, classifier.predict(X_test) )\n",
    "        classification_report_csv(report, os.path.join(dir_path, modelfolder, \"model_approachEnsemble_report.csv\"),\"Ensemble\") \n",
    "        pd.DataFrame(line).to_csv(os.path.join(dir_path, modelfolder, \"model_approachEnsemble_history.csv\")) \n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    time = (datetime.now()-time).total_seconds() * 1000\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    print(\"Done. \" + str(time) + \" milliseconds\")\n",
    "    print(\"---------------------------------------------------------------------------------\")\n",
    "    return time\n",
    "\n",
    "def get_line(y_true, y_pred):\n",
    "    acc = accuracy(y_true, y_pred)\n",
    "    f1  = f1_macro(y_true, y_pred)\n",
    "    prec= precision_macro(y_true, y_pred)\n",
    "    rec = recall_macro(y_true, y_pred)\n",
    "    accTop5 = 0 #calculateAccTop5(classifier, X_test, y_test, 5)\n",
    "    line=[acc, f1, prec, rec, accTop5]\n",
    "    return line\n",
    "\n",
    "def precision_macro(y_true, y_pred):\n",
    "    from sklearn.metrics import precision_score\n",
    "    return precision_score(y_true, y_pred, average='macro')\n",
    "def recall_macro(y_true, y_pred):\n",
    "    from sklearn.metrics import recall_score\n",
    "    return recall_score(y_true, y_pred, average='macro')\n",
    "def f1_macro(y_true, y_pred):\n",
    "    from sklearn.metrics import f1_score\n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "def accuracy(y_true, y_pred):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(y_true, y_pred, normalize=True)\n",
    "\n",
    "def model_rf(keys, vocab_size, num_classes, max_length, x_train, y_train, x_test, y_test):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier()\n",
    "\n",
    "    nx, nsamples, ny = np.shape(x_train)\n",
    "    x_train = x_train.reshape((nsamples,nx*ny))\n",
    "    nx, nsamples, ny = np.shape(x_test)\n",
    "    x_test = x_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "    print(\"[Data Model:] Building random forrest\")\n",
    "    classifier = RandomForestClassifier(n_estimators=500, n_jobs = -1, random_state = 1, criterion = 'gini', bootstrap=True)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    print(\"[Data Model:] OK\")\n",
    "\n",
    "    return classifier.predict_proba(x_test)\n",
    "\n",
    "#     return classifier\n",
    "\n",
    "def model_movelets(X_train, y_train, X_test, y_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout\n",
    "    from keras.optimizers import Adam\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from automatize.analysis import loadData\n",
    "    from automatize.Methods import classification_report, classification_report_csv, calculateAccTop5, f1\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Neural Network - Definitions:\n",
    "    par_dropout = 0.5\n",
    "    par_batch_size = 200\n",
    "    par_epochs = 80\n",
    "    par_lr = 0.00095\n",
    "\n",
    "    # Building the neural network-\n",
    "    print(\"[Movelets:] Building neural network\")\n",
    "    lst_par_epochs = [80,50,50,30,20]\n",
    "    lst_par_lr = [0.00095,0.00075,0.00055,0.00025,0.00015]\n",
    "\n",
    "    nattr = len(X_train[1,:])    \n",
    "\n",
    "    # Scaling y and transforming to keras format\n",
    "    from sklearn import preprocessing\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train = le.transform(y_train) \n",
    "    y_test = le.transform(y_test)\n",
    "    from keras.utils import to_categorical\n",
    "    y_train1 = to_categorical(y_train)\n",
    "    y_test1 = to_categorical(y_test)\n",
    "    nclasses = len(le.classes_)\n",
    "    from keras import regularizers\n",
    "\n",
    "    #Initializing Neural Network\n",
    "    classifier = Sequential()\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    classifier.add(Dense(units = 100, kernel_initializer = 'uniform', kernel_regularizer= regularizers.l2(0.02), activation = 'relu', input_dim = (nattr)))\n",
    "    #classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout( par_dropout )) \n",
    "    # Adding the output layer       \n",
    "    classifier.add(Dense(units = nclasses, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "    # Compiling Neural Network\n",
    "    adam = Adam(lr=par_lr)\n",
    "    classifier.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy','top_k_categorical_accuracy',f1])\n",
    "    # Fitting our model \n",
    "    history = classifier.fit(X_train, y_train1, validation_data = (X_test, y_test1), batch_size = par_batch_size, epochs = par_epochs, verbose=0)\n",
    "\n",
    "    print(\"[Movelets:] OK\")\n",
    "\n",
    "    return classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_marc(keys, vocab_size, num_classes, max_length, x_train, y_train, x_test, y_test):\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Dense, LSTM, GRU, Dropout\n",
    "    from keras.initializers import he_uniform\n",
    "    from keras.regularizers import l1\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers import Input, Add, Average, Concatenate, Embedding\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    \n",
    "    EMBEDDER_SIZE = 100\n",
    "    MERGE_TYPE    = 'concatenate' # 'add', 'average'\n",
    "    RNN_CELL      = 'lstm' # 'gru'\n",
    "    \n",
    "    CLASS_DROPOUT = 0.5\n",
    "    CLASS_HIDDEN_UNITS = 100\n",
    "    CLASS_LRATE = 0.001\n",
    "    CLASS_BATCH_SIZE = 64\n",
    "    CLASS_EPOCHS = 1000\n",
    "    EARLY_STOPPING_PATIENCE = 30\n",
    "    BASELINE_METRIC = 'acc'\n",
    "    BASELINE_VALUE = 0.5\n",
    "    \n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "\n",
    "    for idx, key in enumerate(keys):\n",
    "        if key == 'lat_lon':\n",
    "            i = Input(shape=(max_length, vocab_size[key]),\n",
    "                      name='input_' + key)\n",
    "            e = Dense(units=EMBEDDER_SIZE,\n",
    "                      kernel_initializer=he_uniform(seed=1),\n",
    "                      name='emb_' + key)(i)\n",
    "        else:\n",
    "            i = Input(shape=(max_length,),\n",
    "                      name='input_' + key)\n",
    "            e = Embedding(vocab_size[key],\n",
    "                          EMBEDDER_SIZE,\n",
    "                          input_length=max_length,\n",
    "                          name='emb_' + key)(i)\n",
    "        inputs.append(i)\n",
    "        embeddings.append(e)\n",
    "\n",
    "    if len(embeddings) == 1:\n",
    "        hidden_input = embeddings[0]\n",
    "    elif MERGE_TYPE == 'add':\n",
    "        hidden_input = Add()(embeddings)\n",
    "    elif MERGE_TYPE == 'average':\n",
    "        hidden_input = Average()(embeddings)\n",
    "    else:\n",
    "        hidden_input = Concatenate(axis=2)(embeddings)\n",
    "\n",
    "    hidden_dropout = Dropout(CLASS_DROPOUT)(hidden_input)\n",
    "\n",
    "    if RNN_CELL == 'lstm':\n",
    "        rnn_cell = LSTM(units=CLASS_HIDDEN_UNITS,\n",
    "                        recurrent_regularizer=l1(0.02))(hidden_dropout)\n",
    "    else:\n",
    "        rnn_cell = GRU(units=CLASS_HIDDEN_UNITS,\n",
    "                       recurrent_regularizer=l1(0.02))(hidden_dropout)\n",
    "\n",
    "    rnn_dropout = Dropout(CLASS_DROPOUT)(rnn_cell)\n",
    "\n",
    "    softmax = Dense(units=num_classes,\n",
    "                    kernel_initializer=he_uniform(),\n",
    "                    activation='softmax')(rnn_dropout)\n",
    "\n",
    "    classifier = Model(inputs=inputs, outputs=softmax)\n",
    "    opt = Adam(lr=CLASS_LRATE)\n",
    "\n",
    "    classifier.compile(optimizer=opt,\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['acc'])\n",
    "\n",
    "    classifier.fit(x=x_train,\n",
    "                   y=y_train,\n",
    "                   validation_data=(x_test, y_test),\n",
    "                   batch_size=CLASS_BATCH_SIZE,\n",
    "                   shuffle=True,\n",
    "                   epochs=CLASS_EPOCHS,\n",
    "                   verbose=0)\n",
    "    \n",
    "    return classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'run1'\n",
    "prefix = run+'/Promoters'\n",
    "results     = os.path.join(res_path, 'HpL-5fold_4T_60G')\n",
    "\n",
    "ApproachEnsemble(os.path.join(results, prefix, 'HpL-specific'), os.path.join(data, 'promoters', run), save_results=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'run4'\n",
    "prefix = run+'/Foursquare NYC'\n",
    "results     = os.path.join(res_path, 'hiper-compare_v2')\n",
    "\n",
    "ApproachEnsemble(os.path.join(results, prefix, 'HpL-specific'), os.path.join(root, 'data', 'foursquare_nyc', run), save_results=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectories(dir_path, tid_col='tid',\n",
    "                     label_col='label', geo_precision=8, drop=[]):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    from marc.core.utils.geohash import bin_geohash\n",
    "    print(\"Loading data from file(s) \" + dir_path + \"... :D\")\n",
    "    df_train = pd.read_csv(os.path.join(dir_path, 'train.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(dir_path, 'test.csv'))\n",
    "    df = df_train.copy().append(df_test)\n",
    "    tids_train = df_train[tid_col].unique()\n",
    "\n",
    "    keys = list(df.keys())\n",
    "    vocab_size = {}\n",
    "    keys.remove(tid_col)\n",
    "\n",
    "    for col in drop:\n",
    "        if col in keys:\n",
    "            keys.remove(col)\n",
    "            print(\"Column '\" + col + \"' dropped \" +\n",
    "                       \"from input file!\")\n",
    "        else:\n",
    "            print(\"Column '\" + col + \"' cannot be \" +\n",
    "                       \"dropped because it was not found!\")\n",
    "\n",
    "    num_classes = len(set(df[label_col]))\n",
    "    count_attr = 0\n",
    "    lat_lon = False\n",
    "\n",
    "    if 'lat' in keys and 'lon' in keys:\n",
    "        keys.remove('lat')\n",
    "        keys.remove('lon')\n",
    "        lat_lon = True\n",
    "        count_attr += geo_precision * 5\n",
    "        print(\"Attribute Lat/Lon: \" +\n",
    "                   str(geo_precision * 5) + \"-bits value\")\n",
    "\n",
    "    for attr in keys:\n",
    "\n",
    "        if attr != label_col:\n",
    "            df[attr] = LabelEncoder().fit_transform(df[attr])\n",
    "            vocab_size[attr] = max(df[attr]) + 1\n",
    "        \n",
    "            values = len(set(df[attr]))\n",
    "            count_attr += values\n",
    "            print(\"Attribute '\" + attr + \"': \" +\n",
    "                       str(values) + \" unique values\")\n",
    "\n",
    "    print(\"Total of attribute/value pairs: \" +\n",
    "               str(count_attr))\n",
    "    keys.remove(label_col)\n",
    "\n",
    "    x = [[] for key in keys]\n",
    "    y = []\n",
    "    idx_train = []\n",
    "    idx_test = []\n",
    "    max_length = 0\n",
    "    trajs = len(set(df[tid_col]))\n",
    "\n",
    "    if lat_lon:\n",
    "        x.append([])\n",
    "\n",
    "    for idx, tid in enumerate(set(df[tid_col])):\n",
    "        traj = df.loc[df[tid_col].isin([tid])]\n",
    "        features = np.transpose(traj.loc[:, keys].values)\n",
    "\n",
    "        for i in range(0, len(features)):\n",
    "            x[i].append(features[i])\n",
    "\n",
    "        if lat_lon:\n",
    "            loc_list = []\n",
    "            for i in range(0, len(traj)):\n",
    "                lat = traj['lat'].values[i]\n",
    "                lon = traj['lon'].values[i]\n",
    "                loc_list.append(bin_geohash(lat, lon, geo_precision))\n",
    "            x[-1].append(loc_list)\n",
    "\n",
    "        label = traj[label_col].iloc[0]\n",
    "        y.append(label)\n",
    "\n",
    "        if tid in tids_train:\n",
    "            idx_train.append(idx)\n",
    "        else:\n",
    "            idx_test.append(idx)\n",
    "\n",
    "        if traj.shape[0] > max_length:\n",
    "            max_length = traj.shape[0]\n",
    "\n",
    "    if lat_lon:\n",
    "        keys.append('lat_lon')\n",
    "        vocab_size['lat_lon'] = geo_precision * 5\n",
    "\n",
    "    print(\"Loading data from files \" + dir_path + \"... DONE!\")\n",
    "    \n",
    "    x_train = np.asarray([[x[j][i] for i in idx_train] for j in range(2)])\n",
    "    y_train = np.asarray([y[i] for i in idx_train])\n",
    "    x_test  = np.asarray([[x[j][i] for i in idx_test] for j in range(2)])\n",
    "    y_test  = np.asarray([y[i] for i in idx_test])\n",
    "\n",
    "    print('Trajectories:  ' + str(trajs))\n",
    "    print('Labels:        ' + str(len(keys)))\n",
    "    print('Train size:    ' + str(len(x_train[0]) / trajs))\n",
    "    print('Test size:     ' + str(len(x_test[0]) / trajs))\n",
    "    print('x_train shape: ' + str(x_train.shape))\n",
    "    print('y_train shape: ' + str(y_train.shape))\n",
    "    print('x_test shape:  ' + str(x_test.shape))\n",
    "    print('y_test shape:  ' + str(y_test.shape))\n",
    "    \n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    x_train = np.asarray([pad_sequences(f, max_length, padding='pre') for f in x_train])\n",
    "    x_test  = np.asarray([pad_sequences(f, max_length, padding='pre') for f in x_test])\n",
    "    \n",
    "\n",
    "    return (keys, vocab_size, num_classes, max_length,\n",
    "            x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2\n",
    "def get_trajectories(dir_path, tid_col='tid',\n",
    "                     label_col='label', geo_precision=8, drop=[]):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    from marc.core.utils.geohash import bin_geohash\n",
    "    print(\"Loading data from file(s) \" + dir_path + \"... \")\n",
    "    df_train = pd.read_csv(os.path.join(dir_path, 'train.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(dir_path, 'test.csv'))\n",
    "    df = df_train.copy().append(df_test)\n",
    "    tids_train = df_train[tid_col].unique()\n",
    "\n",
    "    keys = list(df.keys())\n",
    "    vocab_size = {}\n",
    "    keys.remove(tid_col)\n",
    "\n",
    "    for col in drop:\n",
    "        if col in keys:\n",
    "            keys.remove(col)\n",
    "            print(\"Column '\" + col + \"' dropped \" +\n",
    "                       \"from input file!\")\n",
    "        else:\n",
    "            print(\"Column '\" + col + \"' cannot be \" +\n",
    "                       \"dropped because it was not found!\")\n",
    "\n",
    "    num_classes = len(set(df[label_col]))\n",
    "    count_attr = 0\n",
    "    lat_lon = False\n",
    "\n",
    "    if 'lat' in keys and 'lon' in keys:\n",
    "        keys.remove('lat')\n",
    "        keys.remove('lon')\n",
    "        lat_lon = True\n",
    "        count_attr += geo_precision * 5\n",
    "        print(\"Attribute Lat/Lon: \" +\n",
    "                   str(geo_precision * 5) + \"-bits value\")\n",
    "\n",
    "    for attr in keys:\n",
    "        if attr != label_col:\n",
    "            df[attr] = LabelEncoder().fit_transform(df[attr])\n",
    "            vocab_size[attr] = max(df[attr]) + 1\n",
    "\n",
    "            values = len(set(df[attr]))\n",
    "            count_attr += values\n",
    "            print(\"Attribute '\" + attr + \"': \" +\n",
    "                       str(values) + \" unique values\")\n",
    "\n",
    "    print(\"Total of attribute/value pairs: \" +\n",
    "               str(count_attr))\n",
    "    keys.remove(label_col)\n",
    "\n",
    "    x = [[] for key in keys]\n",
    "    y = []\n",
    "    idx_train = []\n",
    "    idx_test = []\n",
    "    max_length = 0\n",
    "    trajs = len(set(df[tid_col]))\n",
    "\n",
    "    if lat_lon:\n",
    "        x.append([])\n",
    "\n",
    "    for idx, tid in enumerate(set(df[tid_col])):\n",
    "        traj = df.loc[df[tid_col].isin([tid])]\n",
    "        features = np.transpose(traj.loc[:, keys].values)\n",
    "\n",
    "        for i in range(0, len(features)):\n",
    "            x[i].append(features[i])\n",
    "\n",
    "        if lat_lon:\n",
    "            loc_list = []\n",
    "            for i in range(0, len(traj)):\n",
    "                lat = traj['lat'].values[i]\n",
    "                lon = traj['lon'].values[i]\n",
    "                loc_list.append(bin_geohash(lat, lon, geo_precision))\n",
    "            x[-1].append(loc_list)\n",
    "\n",
    "        label = traj[label_col].iloc[0]\n",
    "        y.append(label)\n",
    "\n",
    "        if tid in tids_train:\n",
    "            idx_train.append(idx)\n",
    "        else:\n",
    "            idx_test.append(idx)\n",
    "\n",
    "        if traj.shape[0] > max_length:\n",
    "            max_length = traj.shape[0]\n",
    "\n",
    "    if lat_lon:\n",
    "        keys.append('lat_lon')\n",
    "        vocab_size['lat_lon'] = geo_precision * 5\n",
    "\n",
    "    one_hot_y = OneHotEncoder().fit(df.loc[:, [label_col]])\n",
    "\n",
    "    x = [np.asarray(f) for f in x]\n",
    "    y = one_hot_y.transform(pd.DataFrame(y)).toarray()\n",
    "    print(\"Loading data from files ... DONE!\")\n",
    "    \n",
    "    x_train = np.asarray([f[idx_train] for f in x])\n",
    "    y_train = y[idx_train]\n",
    "    x_test = np.asarray([f[idx_test] for f in x])\n",
    "    y_test = y[idx_test]\n",
    "\n",
    "    print('Trajectories:  ' + str(trajs))\n",
    "    print('Labels:        ' + str(len(keys)))\n",
    "    print('Train size:    ' + str(len(x_train[0]) / trajs))\n",
    "    print('Test size:     ' + str(len(x_test[0]) / trajs))\n",
    "    print('x_train shape: ' + str(x_train.shape))\n",
    "    print('y_train shape: ' + str(y_train.shape))\n",
    "    print('x_test shape:  ' + str(x_test.shape))\n",
    "    print('y_test shape:  ' + str(y_test.shape))\n",
    "    \n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    x_train = np.asarray([pad_sequences(f, max_length, padding='pre') for f in x_train])\n",
    "    x_test  = np.asarray([pad_sequences(f, max_length, padding='pre') for f in x_test])\n",
    "\n",
    "    return (keys, vocab_size, num_classes, max_length,\n",
    "            x_train, y_train,\n",
    "            x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, vocab_size, num_classes, max_length, \\\n",
    "X_train3, y_train3, X_test3, y_test3 = get_trajectories(os.path.join(data, 'promoters', 'run1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_train3))\n",
    "print(np.shape(y_train3))\n",
    "print(np.shape(X_test3))\n",
    "print(np.shape(y_test3))\n",
    "(keys, vocab_size, num_classes, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data(keys, vocab_size, num_classes, max_length, x_train, y_train, x_test, y_test):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Dense, LSTM, GRU, Dropout\n",
    "    from keras.initializers import he_uniform\n",
    "    from keras.regularizers import l1\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers import Input, Add, Average, Concatenate, Embedding\n",
    "    from keras.callbacks import EarlyStopping\n",
    "\n",
    "    CLASS_DROPOUT = 0.5\n",
    "    CLASS_HIDDEN_UNITS = 100\n",
    "    CLASS_LRATE = 0.001\n",
    "    CLASS_BATCH_SIZE = 64\n",
    "    CLASS_EPOCHS = 1000\n",
    "    EARLY_STOPPING_PATIENCE = 30\n",
    "    BASELINE_METRIC = 'acc'\n",
    "    BASELINE_VALUE = 0.5\n",
    "    EMBEDDER_SIZE = 100\n",
    "    MERGE_TYPE = 'concatenate'\n",
    "    RNN_CELL = 'gru'\n",
    "\n",
    "#     num_classes = 1\n",
    "\n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "    for idx, key in enumerate(keys):\n",
    "        if key == 'lat_lon':\n",
    "            i = Input(shape=(max_length, vocab_size[key]),\n",
    "                      name='input_' + key)\n",
    "            e = Dense(units=EMBEDDER_SIZE,\n",
    "                      kernel_initializer=he_uniform(seed=1),\n",
    "                      name='emb_' + key)(i)\n",
    "        else:\n",
    "            i = Input(shape=(max_length,),\n",
    "                      name='input_' + key)\n",
    "            e = Embedding(vocab_size[key],\n",
    "                          EMBEDDER_SIZE,\n",
    "                          input_length=max_length,\n",
    "                          name='emb_' + key)(i)\n",
    "        inputs.append(i)\n",
    "        embeddings.append(e)\n",
    "\n",
    "    if len(embeddings) == 1:\n",
    "        hidden_input = embeddings[0]\n",
    "    if MERGE_TYPE == 'add':\n",
    "        hidden_input = Add()(embeddings)\n",
    "    elif MERGE_TYPE == 'average':\n",
    "        hidden_input = Average()(embeddings)\n",
    "    else:       \n",
    "        hidden_input = Concatenate(axis=2)(embeddings)\n",
    "\n",
    "    hidden_dropout = Dropout(CLASS_DROPOUT)(hidden_input)\n",
    "\n",
    "    if RNN_CELL == 'lstm':\n",
    "        rnn_cell = LSTM(units=CLASS_HIDDEN_UNITS,\n",
    "                        recurrent_regularizer=l1(0.02))(hidden_dropout)\n",
    "    else:\n",
    "        rnn_cell = GRU(units=CLASS_HIDDEN_UNITS,\n",
    "                       recurrent_regularizer=l1(0.02))(hidden_dropout)\n",
    "\n",
    "    rnn_dropout = Dropout(CLASS_DROPOUT)(rnn_cell)\n",
    "    softmax = Dense(units=num_classes,\n",
    "                    kernel_initializer=he_uniform(),\n",
    "                    activation='softmax')(rnn_dropout)\n",
    "#                     activation='sigmoid')(rnn_dropout)\n",
    "\n",
    "    classifier = Model(inputs=inputs, outputs=softmax)\n",
    "    opt = Adam(lr=CLASS_LRATE)\n",
    "\n",
    "    classifier.compile(optimizer=opt,\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['acc'])\n",
    "\n",
    "#     classifier.fit(x=np.array(x_train),\n",
    "#                    y=np.array(y_train),\n",
    "    classifier.fit(x=x_train,\n",
    "                   y=y_train,\n",
    "                   validation_data=(x_test, y_test),\n",
    "#                    validation_data=(np.array(x_test), np.array(y_test)),\n",
    "                   batch_size=CLASS_BATCH_SIZE,\n",
    "                   shuffle=True,\n",
    "                   epochs=CLASS_EPOCHS,\n",
    "                   verbose=0)\n",
    "\n",
    "    return classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"marc/multi_feature_classifier.py\" \"../new_data/promoters/run1/train.csv\" \"../new_data/promoters/run1/test.csv\" \"../results/MARC-specific_results.csv\" \"MARC-specific\" 100 concatenate lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
